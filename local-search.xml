<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>末流211CS入营清华大学保研失败经历</title>
    <link href="/2024/03/05/%E6%9D%82%E8%B0%88/%E4%BF%9D%E7%A0%94%E7%BB%8F%E5%8E%86/"/>
    <url>/2024/03/05/%E6%9D%82%E8%B0%88/%E4%BF%9D%E7%A0%94%E7%BB%8F%E5%8E%86/</url>
    
    <content type="html"><![CDATA[<h1 id="末流211CS入营清华大学保研失败经历"><a href="#末流211CS入营清华大学保研失败经历" class="headerlink" title="末流211CS入营清华大学保研失败经历"></a>末流211CS入营清华大学保研失败经历</h1><p>2023-11-08<br>本文转载自本人的知乎专栏<a href="https://zhuanlan.zhihu.com/p/659272255">https://zhuanlan.zhihu.com/p/659272255</a><br>图片不能直接复制，想看原文下载文末PDF吧，主要还是怕知乎原文章哪天被和谐了<br>失败总是贯穿始终的，这就是人生！但是打不倒我的只会让我变的更强！</p><h1 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h1><p>朋友今天说他要写一个经验贴，我当时正在外面散心，回来又不想看考研政治了，于是顺手也写了一个<br>我的结局是未拿到本科学校推免资格，保研失败。三个月冲刺上科大考研。心态放平，做好二战准备<br>本来都找到长城汽车的自动驾驶部分做算法实习了（已拿offer），这下实习也去不了了。目前已经放弃实习<br>关于保研常用术语，可以看下我朋友的保研经验分享，我在这里不再赘述</p><h1 id="个人背景"><a href="#个人背景" class="headerlink" title="个人背景"></a>个人背景</h1><p>CS方向末流211（客观来说，这里不带吹黑成分），CS，学科评估第四轮B-，第五轮听说C。<br>夏令营rk 47&#x2F;108（42%），好像可以开一个30%多的，但是我属于有个成绩证明就行的那种，没参加预推免，最后好像是35&#x2F;117（29%）<br>英语：四级426，六级没过<br>有用的竞赛：机器人国二，Kaggle Solo铜。CVPR WORKSHOP某个track的top 5%<br>没用的竞赛：美赛F，两个蓝桥杯省一（省一以下默认不写）<br>有用的科研：六段。智能车实验室两段算法落地，大数据实验室两段（一个偏联邦学习一个偏变量选择）生物信息学实验室一段，多媒体实验室一段<br>没用的科研：无，科研都有用<br>目标方向：模式识别，自动驾驶，多媒体计算<br>目标院校：清华车辆，中科大先研，计算所（先研offer了，计算所有两个实验室霸面入营，冲突放弃了）</p><h1 id="夏令营"><a href="#夏令营" class="headerlink" title="夏令营"></a>夏令营</h1><p>我的情况比较特殊，因为预推免老师考核学生的时间较少，我想同时兼顾还行的title与不错的老师非常难，所以我只参加了夏令营，加上夏令营拿到梦校，预推免天天担心名额，最终也没参加预推免，我只报名弱<br>5.30 中科院计算所VIPL 收到霸面，入营VIPL 与中科大冲突，放弃<br>6.1 厦门大学信息学院 收到推荐 放弃推荐<br>6.2 中科大信院 意愿收集 被刷<br>6.5 中科院信工所国重 老师直博offer 院面被刷，英语没答出来<br>6.19 清华智能产业研究院 简历过筛，课题组一面 放弃线下两月打工<br>6.19 中科大先研院 导师终面，支持转博信院。当天下午收到推荐，通过学院面，7.25口头双选。9.21收到正式offer 没保研资格，第一时间放弃，老师后续招生顺利<br>7.1 南开杰青组 通过前两轮简历筛和组面 被发配小导，放弃考核<br>7.18 中科院计算所网数 收到霸面，入营网数 我已经在中科大了。。</p><h2 id="中科院计算所VIPL重点实验室"><a href="#中科院计算所VIPL重点实验室" class="headerlink" title="中科院计算所VIPL重点实验室"></a>中科院计算所VIPL重点实验室</h2><p>老师是VIPL某年轻优青，我与老师本科出身类似，老师很爽快发了霸面，老师每年都会收一个本科出身不那么好的，最后因为VIPL实验室本身竞争太大，而科大那边又基本上是稳offer了，最终含泪放弃<br>笔试五门还有机试，可能性太小，放弃</p><h2 id="厦门大学信息学院"><a href="#厦门大学信息学院" class="headerlink" title="厦门大学信息学院"></a>厦门大学信息学院</h2><p>厦门大学信息学院联系了生信实验室一位教授，他们做生信+CV，学长电话面+编程能力考核，比较简单，最后因为不太可能选择厦门大学于是放弃推荐，老师后续招生顺利</p><h2 id="中科大信院"><a href="#中科大信院" class="headerlink" title="中科大信院"></a>中科大信院</h2><p>这波纯被海了一手，没有后续，备胎的备胎，但是我很想去科大，当时还是高兴了几天（老师组很强，我也配不上倒是实话，当时有人联系我我还震惊了很久）</p><h2 id="中科院信工所国重"><a href="#中科院信工所国重" class="headerlink" title="中科院信工所国重"></a>中科院信工所国重</h2><p>与老师聊的很投机，组里小老板一轮电话面试+老师本人两小时腾讯会议面试，基本上确定导师offer了，结果被国重给刷了，tmd英语我是一句没答上来<br>老师后续招生很顺利，毕竟国重+杰青组（老师是原cxc组小导，现hqm组小导）,腾讯会议聊了两个小时</p><h2 id="清华智能产业研究院"><a href="#清华智能产业研究院" class="headerlink" title="清华智能产业研究院"></a>清华智能产业研究院</h2><p>6.19是我人生高光时刻，清华面试，中科大offer都来了<br>清华是项目制度，当时投了一个自动驾驶的项目，在池中里被捞了，不过剩下的是开发部署的职位，加上线下两个月打工，offer概率太小了&amp;大概率给机械学位，放弃</p><h2 id="中科大先研院"><a href="#中科大先研院" class="headerlink" title="中科大先研院"></a>中科大先研院</h2><p>我有个很厉害的学长在先研，能带我飞那种，联系了一个六系优青，说明了可能没保研名额，老师还是把offer发给了我。面试过程中问了我想科研为什么不去信院，并且支持转博信院。通过学院面，拿到offer。没拿到保研名额第一时间释放，老师后续招生顺利</p><h2 id="南开杰青组"><a href="#南开杰青组" class="headerlink" title="南开杰青组"></a>南开杰青组</h2><p>很难蚌，过了前两轮面后问我去不去南理工的名额，拒绝后被分给小导（搞自动驾驶方向的），拿到中科大offer后放弃考核,这个组最后穿了</p><h2 id="中科院计算所网数重点实验室"><a href="#中科院计算所网数重点实验室" class="headerlink" title="中科院计算所网数重点实验室"></a>中科院计算所网数重点实验室</h2><p>联系老师的时候我觉得回复是官回。。结果入营了，当时人在中科大夏令营了已经，只能放弃</p><h1 id="预推免"><a href="#预推免" class="headerlink" title="预推免"></a>预推免</h1><p>夏令营已经有华五offer了（可能很多人觉得先研是nm华五呢，但是我中科大的狗（划掉）那边有学长可以带我，感觉跟去信院没啥区别），预推免就投清华北大玩了一下，陶瓷都没套。最想去的清华自动化系企业联培因为我们学院降低保研率最后也没关注（不过我tm也去不了hhh），整个预推免等于没参加</p><h1 id="关于面试"><a href="#关于面试" class="headerlink" title="关于面试"></a>关于面试</h1><p>我报名的学校全都是弱com，一般情况下是面试+考核，考核就是读论文做项目，这个看自己能力，本科有科研经历的话比较简单<br>面试step如下：<br>自我介绍（一般导师面为中文，学院面为英语）<br>英语问答（学院面一般有，我在绿裙发过一个英语面试问答，那个就差不多了）<br>PPT介绍成果（学院导师面均有，写重点就行，学术无关没必要写）<br>介绍一下你简历上你最得意的一个项目（我都是介绍智能车-智能视觉组做的东西，智能车竞速类非常有含金量，实验室竞速组的人最差也去中九了，非竞速类p用没有）<br>简历问穿，项目细节，我做的是落地，问的很细<br>未来规划（不会说就读博）</p><h1 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h1><p>保研最重要的是什么？是有保研资格。有时候为了保研资格，用一些令人唾弃的手段也能达到目的，欢迎对号入座。<br>借用我朋友一句话作为结束语，他比我文采更好，我是一个不太会写东西只会做实事的人</p><p>希望大家都有光明的未来！包括我在内</p><h1 id="原文PDF版本链接（含图片）https-kdocs-cn-l-cdJ9H9OQZZy6"><a href="#原文PDF版本链接（含图片）https-kdocs-cn-l-cdJ9H9OQZZy6" class="headerlink" title="原文PDF版本链接（含图片）https://kdocs.cn/l/cdJ9H9OQZZy6"></a>原文PDF版本链接（含图片）<a href="https://kdocs.cn/l/cdJ9H9OQZZy6">https://kdocs.cn/l/cdJ9H9OQZZy6</a></h1>]]></content>
    
    
    <categories>
      
      <category>生活</category>
      
    </categories>
    
    
    <tags>
      
      <tag>杂谈</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Fluid快速开始</title>
    <link href="/2024/03/05/%E6%96%87%E6%A1%A3/README/"/>
    <url>/2024/03/05/%E6%96%87%E6%A1%A3/README/</url>
    
    <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="854ccef350866752e36bad7ee0252937c7b5a83c77d4f8eaca5d240cd281f6f4">4c7d53edba4f7233f4dcfda30a30b6935cb6dfb37e53dec01fd4b04ca04aa3b568483c2854573f489c7293d481e8877423130fa94958a0fbbb7438d463bf8edd1919fc6adf0647ce3528f35f21e8544011017a15ec420f738bd488fb238e86da92ce36b0d66340cc0aed488218a8247e0a450886037d22ec2b2806657f524d0c64ab856f2afc2fce53e6432744d8b955f898d6447177e4eb63e0e83749dacf9d656dfacccd1d9f55f1b98e017e3102f03cc5c05dbd199758d08d54db77e1873ce6725e4fec648b1eec4444a852ed75304ab032be8286bde4c953303cf3b7c7bb976fb2d51325106f4acbdc9a069f8d806de7a28ad2bca00c8e663429487049929bdea1440c81df8bf83e1aa0a3df134b302639d34c53022a94b58bd6b92781785d37799b10372501a3f0500475258db21397e073d76ba2b7ad0e414e4a7e83ea109fcd38eb0ca4c409d836331563df40b8bdaa249941548b7a2953c0be31c195e43f8408f1d0d4b95f70d4211b0ec0fac05fa3b382b57cda41d2d1d9bb602831809192a2e2508078f36a529f2ef5f0e46256a29fce3e61086948504053a78dd5ef79a3bb67410cab16ed3a84440437daadd797d04c8388535e232e9529fccfd9258906c48cbb56268e310ae6d6dfe12ac68edfdfdc4eb3de882294c48fc3541ed7a05cdbf2a224c1fad02d5311a4bdb2015cfe6d2c96b8a627b314b405efda203d3da2270865008bd0d0a0425b1f1f46b7d9f25f46bc74564f7dcd3966920e46df608f11ea51e58a4f130f9fb13620c68ad6afa15231e1b356201ab8feb2a4e8b47f2f278ea6da951823547277049764f82b990c5724912702f590da3c0f321dd71a3c6c5120f2f2394d3c050c677a30231a4c4f784ec77af58edfc6a91c8acb1f13e71dee2141f0a0d9822db914e26369272e7fe11d3cadfe86c6bdf09412449571cda821700753832eaddebbf21f32be413e069ae2f8c037288d0f01d9fe89702ce0768892252bcdcb42fcba3b3c59156fc23ae49c6d3d1de018a798a12e32a4084ce0ca2e8532f728f616ab6ea4fc5980b1bd78a72d76c38d43ceda7a23be7fe423df47191cc2e210a133fc8d307e08ce9ab87d2699462ae309173ecfd82aea2f2d480e02ab99328453b9b1b68bac015fe2b0c7dee8c1c3d6bcbd15f7d2b6dc0d0fc85b51319b117570558ee1fa476d2f42d05bb9981289f8dd2be87abd500c7437b4fe517ffbb4a9daf6d017c6e13231bfc5293616002626111ae9635dcda722f3b1a881e50dde487bef0d42a0a33e7d4f49af3f83b115801204526167d8399fffb022ae341ac44307bef168f47500d270184a87161a7861f70575e7a334f62a83366953b0b05be2a9e29136ceec92bd89a79560e72639844f571834d925ec9b21aa50c3606a5d56e00fb86d8bb0949637606096eba2fdf26577330fa9303d512d9ed729c60a7437aca44b50f9c13b21a6585b71d4a45f5682f54906f4c8ac79ff43e629ae43e5a121bf434533246a29eb4011aaeebce720b57762a409c0a4fad5f7f21d31ea1e82e3ec472689bf540de910a16eddb94389ea6fb4c328a827179ff9564ca04c9e40e6767187d621929e5ac7764f09eb2732ede766f3b2fad7ea2a7566e00c962d98a861d8760152cdb51926e0262322e31b0638eae3d354815cdc3b6d9be71c84cd1c0c61e85eccd27bdb0c715aafeccb69e17cca6c752e76fd4da156c5c8c9db1df455f55b55cee5f8d98c766ccf6c84669f4f482a14c6639ab2c045cdc3e14762f101a3600f5f23233966fa714643f499ce83b8a8935841fc1213eb8e39d60e4164a3428f3a6195aedae9cf34f732233592ce3ef3d48f94a6e9e566b02c0cd72ced2ee5d0d31bf6b8757989d1f4b356c56531c59a623437837e8d03e00211aafb8b190b55de46f8066306006adcf3515335d0d5c0d5112d99c44fbb82c3cb10f2c8d5713165c941772f0d887173392a7f8b8793beeffd569ed242b957ecac1e34aa0d7bea6cc69542da3d6e6e05a4355c67cb31e7addb3e4d1bccddbd9460f53ef82f5fb5296bd5f8358dbd5a45e7f848c96f5f542f9fa5ea9380ff6609717f0f9712641e59b45b2a89a69d5475b0482b72d0e0df94a9e69aef1c8ae626504235bbe601f24957a3718daf0c56f6c2131fc9d430b4f097d64914cb962440e5dbaaf0ebfc7d858459ff4bca53f0a2568c8bc28c8ecfd8015b8c77f97991cf0db599c0e8e0df26a0d83705d9f070e09891cddeb63c815990d8142747438dbeb24e5e4e0d6aeac04bb8c7243a1ee2137a8b31c245ab39d23081a46c0de4d2e87ac9f5b265e3e7174940f65b6d45bc6f58abfdd060942e7a80a956130074902dbf6a1584e4b6e5c7ed0ea17988f70017c7dd7904b5a134525bfa17e0aa2983c83bf67541d99863e7f5d1a98f23d1ca6725d9d92232debc9937dd80c614ef56cb3a32100d176793d9d6fc75f0ae82022997456ff2778ccf75269c2b4ccdcf8a5684befeb35fab3a2b13532c5117e0422527e444ea3d0459c570532ff161d8ba6c74717548340f542181843a51a2848866b08fbf54250487081cdb11c1541f8df2997d0478eb088ecf049d4bfd1a237da7e53a3b9e7c131dcce41805b16c36499981fde05c4f73a36fe94a1d93294fbe2268130f806879f796805a772aa5c081125dd3611638a26755ebbe0517ddf4ec5ca208db2cefd20b8b5e20f07d4eb954197ef40f5079c57bf87ef214576c2e6def94c29a1f1fd3e93403a36b25d29aff98e7870f8fc7c00a27da26c2361fad1ffc1510f8c9ee8f486a643902dbf550f61db27b821102c3b058aacd3a45d0ec7bfbaf8549d42a0b5e8abec4b110c07f9d46edd5ba83b3604931970f826b2213f941c92db498af57845d73e89508363c7ffe3d004abf70ed22bffcace8aa3ccde2ad2f5187ca7deb919e51d72991e01236c057ce392a6ac0ba34940d2d748dd2b3b7437bfc5e87ebc5b03a739e31a2fe9c55b71df3aec977ffbf1d20aa13cf5892ccbd56416c933d4709a7b20c7c220399ec76caf4bad6fb575be9644cc937dc78881cc4d6b40b2f916eb505ff59f4d84a3258aa21ee7c3ac2691d42a9f05dcdbbb2a91899c4516cfbfa51c9eeb9bcddfa3ec68bca1fff419674177afd23ee12d50a1244e31d117a45ad33e5b714647a841cd07a55a34caddd96a42f15718d80c64b6ecc0d535ecaf9695d29cb2b77db32b0beb6aa75b6c44e2e8442ba1fa0e9d666a6d498a6b27dd45af8be1aac18245bdc1887ede25f886a2fac4744672885aa87491132d4352c710c4c6c337d61c3c822cd58cb59a9f37e0c408a04e31ba8c367e8b8192406c6ede1396af42befcedd27118c833fd677ced9c830c0e1333fe33258c7f539eb04bff4b13e3d3be396e4b52dad08d3c553893fb623f6f227d00471db9cf659b6bd2f818321b8126f19526e412341a96bd80b54ad5ebf220822892211b3482f6705f9a133183ea44bb45684c3e72d3299062b4be25f34ace00545dbda730124aa9931121faed99fc2a6d2258b6941f4fcf180b797b0956824080ebc32984fbcef03c37dd7441cc3a9a4ecc3f9fa643d7751a3d77719cf8af7a829d0f0731d3f88e053e02517459f7385a88ccb98703128bf6241776865da3ca12b6ab68ea0bc34305c22bbb2800e9c996da62b3f2e7af934e6783e4d613eb1b4a3d9ca9947cd276341425ea4933544fc3c132c4eaea6fdf29246ba8842484d8a9ab1c787ed154986cd87ec6eec35c7b6e6cae78089f4d9e0ce2f5faa6cf72486f8fd08257ab3b767d49351a31d8805f1060d74e4214a116d9e857d016e52235c64c1f5df7b1e71081c3193e73d4bbb249ed3b8a5567cf8f8fb9631be6b30a3a17f313e487dff52d963115e8635c9e5f3c67eaf26ae60ff8176d59240b97129cb21174ba2d6bfb645d68fe897b6831ea332ed203ea33736709dcba627c039724945c0aab439d1e7b543a5158f97649c67ab41757971d649dc1eec2ba6486c7c988d796de60e84b3cd0dfe48771b85fab9ffa495c1b4d50c9eb3e7e9762c810ba61e7134781c70d8ba1d1df475e3baa6f920f38443bd52dcdfe629eff9e5a76f4b97b744636d5345395fe1a8a13527b019afce6020a5cd312220acfe5efd6c648cb8ccea8ffd8a13f28ec4ae712a909614e6fa3b167dde3e9f3334307b587a19054ba5176e127117e1ae577f50cde16c945a770b6c9813c1f287db5716b64715856a118b028626d9d708fdaac1a3b86ad83b75260f0d1a4951d38feafdc03ff70eb32028d08362258dc1118f6dd6cf5238b47f3cca9fa5c431aa58144c3c99bf802f4e2b96a4ad4f25b9f4c1f82b9c6d86103fe491ea8b60fef669f2ecbf8e7b216a2935237166e307bfdf37f986f4eed7225f5c0eef10402447eab31d9adf2895c1b03ad31fd444d84ed26f35b646b11e27d1a299c273622a8ec04a77ac399e2ec98f990255e99ddd2b0d4c1e808ecadc073d7040726866a95b6d56f1a1e8f7f9c13f625f785646e62ab6e4d0048d558cc5ad4d900df52b134af0ebff8b3f10cf4d8c26fe3d6c70bcd5633b90b2178f4a57780a20cd1458f5d06c3f02e3e2eb0bc3d72faa45f17f6080e7280032fabb58446d87934d4cd9469b4f8de1ece162f057cbcc6d87634eb0d5e7aeaae6aaa7c0dc35bba8c793a27fd4748e451b34b5cdcc7cf9f955af96facfe3e6b058d2cb44e432b1c1af9c4956e8d2426a106d683044ce43a8f5c28284d87b2829a7a4ae8d5a18d5d3f723487c955df1e87e48439ec10b963c387bebc402fd881ec7d7318641a581e2d7dfd03375b9a4bdbbe9355f1bf21ac8472014134bf681f0d3fb76ffcd6316865663c25bd80ea67c9ce3d4fc30e371f791bfd72f855b6e8669eee0c91cd16f030b50865f2891ebc42eb587ab1e242e2c1e8371e6480ccdd9cfcce489b67edacaf51f23b57d7f9276df7cf48162421b7afb00f9e5f7aee85670f7a8afc8379d405f45f7b98eea3d7494b80c710dfbe10919edfcdc13ae0e59e0d9a45c42c9fecd142ba0f5e7b4a2615e696c9ecc9f410aa601145dae284b01643cf71edb9335248afbd35917a3b8c2cb3d67e16c44f8a06332f0e8157ea0488243e3bd6e2ba866a756d26a472e114ff4627b0a12bec0b37b059dd7f1766612bf93d6313dcea98022534745590acc596808cadbe457d00c9bc4246ddbb5905654e0ed0aa2d44d4e337390329176ed29086c88a8679e9b474100babb1aae778310a10ff5aa597458a6cca95dcf0d7958f991cf3031314b4e78cb6120700b88a39160cf0f090b1bc361bfc60dd39b36f8ce066d99eeef3713c7ab58be9fd22c8583b86b11d0a10fe31ee674c75381df61a3aff3ed45dbeae4db0ccac420cbd3a587e262235befd9daab00b95757b1b541651efb308ff92052472db1055ce980da888980663ded5d5f4ca5fa13eec576698807e8c0b57d9f458c24c0b0d5307f5ed886795d1a7676af9b052791beda2a80e0fe439da1bf2e755bfad482818f57fd7bd2cba08f3bc067962f60fc3f51acbbe686bbe8e69408fd2b8a8ecd886125be574df83f871036b56bcfbe9c1cbc13ad798c2282dba9db55e0cf1af3f69a625cf1d0061036a2ca42d13bb6c7ef9574412b213fbbcc4477323a0892e4ac7bed51c40e2c5207bd6109d7d8e80074065fdb035db262851975190fcb6cf5342ddf9e6465e1846f651d50fc36bc4e63438313546ab43f1ddeabb4c6c1a82b9f173ec84fa485947685ab3e353d590c95cde25eaa8b177e394e3d4d9bb42ed814667d9b8fb4df106b9ea99c02dfab858aac93f505bfc1e225262ea01fb745e6fa25d6f0fc8090cc2570e6fc18e2e9d825f3b182ac8f52e2cef3e1f8b10003944d211661683d0b3be04b00141792cf135d56254f7ec874bf999652c34fcf2609e93164f2234b299c1a581bf3befee8ccf3823cc0d157c19475a42da4b37df6aa6c9fdd4131e308c03ed2b46f6a6a9fb5e4f3e3db2494a79321620008b50c47a6d6163437a27961a857eacafd9929b16b2127bdf21740b82bd988ac6ab95e9a3f26e632fa1d4daaeb0c1f207b61fa380f664b5d9be7bdeebb1f5be75a5d2da65afcc500f42c2d39fd64b8e78a4f3382eacf5b7a6162cfa777d674ddab49e2b2459f4837fd9ea2a823fa7925a8caec28484b709ebc40c6a840b5c252948450440fb65536c914c46253bdad711c6cf15e11a35741113a39e071a539aba2ee4063ea6ce01c0bff0d17553a0072ab39e1f538132355d7581f3d032dc5a28f4cd88a55f05e4285e212bea8ad695ad3cb136f1f06bdaa4d454d402ec2ff110848e995178d0eea7c337a37434f11e7d208a2bd52c0587c6d99369e537cbd99db53cfb12b4c6eda2203558840d2989e6ede4e1228b83a661708a18c425a5767420d04e877e19623e2e9e10bfad11c071dc243852a1b260e8be9cf68a332d79b2945c0af8059f3da7ad5f7b34b93f1d37a812ab58b019ec7d046ed368e01502111199f395e5cb9d0ca3f497677e4015c3021f9dcd5c4816d958a2ad369080669ec24bb8a007f37b75afd78347de6982c88dd288cbb85eead1d7d611017793720b0f716a5e1d38c0c6a6fe1811fdba7f187a16aa39d561fb0d5305396c6e8f0d0546b86f79ec4ca53531d8be9b85a5f2fc90284a4f0bde1f509b18ba717d478e78b42caf8dcd9cbba8cdfeb37075ed27177432e51627cc8c49851d2e42f0d24d77454a96009e05776b5c929739149c01c058659a49db9b877a4c2aded6c552e297c9f1751848ee5ba21985be1ad284d19035d08ba03100e7d9b9fea0e596844e1dbde6b6e099dac7434ac87c64b2aee584b92a9079b19df6a6303e06ef3d6ba72b039531304a41417d58997e0c4fc650e41294c2baa124006badf7278794c16d514aa9f9a5ef0332c8a532f931b62fcb153b428432c44d1a9813d4a973b031227fd7963a1a80a68e248631735065c4b19447ea99dc1a0e66341250a94030870092444500ddc265f8754eb2c08acf692b70d7b2a79355475cd9a478a27e8d92ef170af1e1cd202e89906b75a941469d0b23f63c51ad97bf3ad2d0bad60851661b96ff883001a87eb2ecf678eb03c68e8fd279f3a403ba4bc37c58a97da52d1737d6cf9beb6e50c2f638459cd82b900f0390f80dc67494aa3e07923f72769418ad7eb87505c7ece94447dee67ca689c4d3ac5f729b2a9931f52b47bcd2de910d32994042dd8f0186e8b046b856f4d2a469d0400e20a59725a729016c1b2b93e09454ca806dbd437cf4d3a201ac072a3ca3a22a0320fbaf310a78f79894e3c96f6e070c007ade368a757df9de3a533d9f4d7a083e0ed290a080bf269dae414a31816aabddff73e1eed6c0e005127aecd3f69c3e9551d6eafffbc1e0cba7f77807f80f098f6325aa977d2f96649ffe05ca6677712c01da462d4c018224c09a7d1dd095cbca78ccc633a3d90654351d469fe9ca1b580cf955aa0b353b2d19ab6ad858c622b18b6b235e249080dbef23a7f476aee88f0c9c04e6c5bc988156b4a022467fd3c656de1729bc70e6bbf98d70d20ecacf5c27ed6fbf7fa6aaf238a28ee92eaaa95ddefa8a8f51ffc065661cc5b38a9e97dd50f0e4659f91e7cbbaaeb41adb106002bdc99ae89404d37e5ea9c10f5fa980f6b88519a375dbacfee1b53f5522ef6af5363d4e6f328c6314e9e4f0031b4b0f62a7b19b122e3988bfa279a152d1a828fb62eb56434ace3911b3f4c13308bb9985899da5608fcbb908511b594d7e1e540858c284bd3c99259c885949843ae568699b61b23919266207d962faf4e709eb82de4a9deb9c68b9b4f253b039c96d56f63b7e81c051215acf8630f2430cc79401292af14ce593e72000ad96c8f04f3bae78419c20893b2ce10815404f0f406d10c857147657ff2ece359c77900ee0f3821e4bbc07536a6e6dfdd3e32b8f64549d21d2502a07758c42f341e4b44c866dbd0493b6ff8cf91d698f2e187712bca2523490ee4824ece5ee7f24d6694ad6b57c163d23b5a36f9bf5a8b6a433d5f65c98f5bbdf28cfc7770db4c5887e4635a6e5cf6e1e337d99e451313720cc6ce5d944c9137cf06bbcc55f313806dae71a25f488454915b05d719e9270447ec773d04c8fa2490c30d5fbe97fdde8fd8d39db5028171cc906778ea50ef2fad5856098cc4e193a6053a12200d2fb7dd4bd05b9765dd79186276a706446c3a359fe5819cebc1d91dcebb623a5c3dd92c6a629c28e852fdd9e8190742d708a03f8d159f6fb490dd2d5114604fe58e50d61f5367da618e501bee2fe6ef643876e0d4bd33bc2c91555d02b90fb090a170dd6749c29e10875c864d518cb2042de8ee7c9254e4a045f7e701f34b5b56247c7eeb57d0da0e6cbf81392a2182efde68f44b6d523b61d79dcb06706ba0afa4ba433ec6ec9732617ca7794549fa349fe803ee2ae2ed65741ba24a15dfaaf658393287827d6de43067264e6f02f3b0ed147d8c2e7ab1e53adc5c295ae0a28b9b819db5c53b26c129145baa4b18d95326b8bf2078aba7763b36961d849b6e452c864b8830ad41bf16e1c4e404e503022e5f8eebdbc668228f52144cd252d327fe7b3384120e378f4715967477e75db1af5fac3468dbd871b2da97ad002e224b5e26dc179cfcb29b8caca79e25aa9830c8ecf1ace655d0da25d82a54b6013f4db9f30c50a3dcf39a326e0e15e4d1faaca149896a10bbb37d54a0334d293cdf60c30be9e22ba3c2e526cf9d88042c7a189319a802b7306d69323d5a6176b6306088f8f3f4aefd034d85c201a2b75948712c490bc2c8285f998e0118bca28894049d9c41c96a61a005355adc471ead789f371cc97ad11938245120e86d9f1f72aff6bbd34114fecaf44168be8b1b16b0b1e3b3dc5c31e447d6ebc3cb923fc549cf1befc6ba241e64b851ea129b8db240905c7b55f9a1df4ddcd255865b9273d325d23dcd800b5911241a68b3b4c43d356662fe43612d824543e885eba6b3e9f96fc0c7eed6f70a7ed812212208eee55a09fc524397f99aba9a1f6c5fce02fd82c908fbdfeac5ad8ed00515a9f894f2030dd36046f6de3e34a6e3a6adede9cd5d1d03e832479268026fa08a2ec2427926d47399f836e412f526422edafff6063d3c4ab41c099d1fa734d472db9c7476695e4c352edfda04bc2e38a7e0368576d3409ce35c3a11b2b4fc4f57c1c8bc5835ca92cc8e5c7873211075d76f9f9a6eb8d9862d1b4998853c870dbb63a20466a676afe17e3a4d254967cc3a1bf8597c948b4e0fb190dda8f1bf3e06a4b7dd2df75279de97c2bf9ca413d2baeff4d6542e88e18cd62d3b7f26bf7ccbe3001661a24e3050873b113eb8ccdcfc89208cd8a652f05570225948a28be66cb8ddffbff9a52d52f43bf7623e3f41f1a426d44d01bd33511f652e0a92a85d198d6011d34ff2e7d5efa9bff5618d0233bad881dbd24a07345852fdf8c0e07698632eecccbdb96c5c43d2f34a8fb4c7a9d34deb766d4d246bbf48519b139685adf0c81b78cb9313c1dfb115f0440356b6b20b12c3bab3985723e55cc9f04f7abe7cc03ae7b7d5c2b6c0b9944d5e013e618f83e118975f31823f56c9629974a1d0b93f07a90c19c56ff2c2eb6addbde2a03b97d90d711e40e5c5a8c0b6bfe1d35ef39d04909adff50a23d57229bbd03db8ee17488aaf484e4edcdac79e175b0e604dac266efad093955bce30edffb9396c249244962baaca54db46b322a6fec3a6a5fd177c68168db8a47eaadf4431780e09990b9de447625e6619e12a588762e98815357fdda4e5c0d8577c3f2c906adceb768485794921ab1c382eb22df5809bd52601f20c618692a965d6169c8ca31a8644266ccff929106a28dc454f889b8c5bbd867fe9f10e851f889f956a9f254e25773c6275a95d923d90fcc221071cd40702ba9829687d9894fa05f72916040d0d7470926ecea5b2efe77f6a7767259edbdb4989075732fb0699f70fd15c3ded24e8aa08d2147ce5455d40484714a04354525c026ec6b4ce702f7c39e7a097ba08ac155788656092e999598808581941a53aaeaad5b8f157abbdd672052036f98e37f972f896e79805dca13c0241da035ad0b25c476450cfcb3076f125365193af208a58822990c2b1cb8389c696d17fa1266844abb8ca9c1f60d948ec56a1e266c5825aa282efcda89bd267c887a857f8a41eb5daa07e7420fc210375e518eb03889f87d69f8a9ae5130645757ee05344edbee85a61a934db52dffe03a40e4623095d205bc28bd46da02e782a2ad79258b189ece451aa50b47e91cc92765e51a2bf36d1f37463b2dd71881b8ca2ba7bc3b03270dabc859555aa292ab9c251b6462bed6e082b912dd0332de7b2f7aff3cc452ba188c6e5cbdabae5cdc559eb700c6cf1fd1b3a6eb89b117fe82b42306db84e4c9f2b4a1e1996251935f25e004c6267c131fa89d363e5a37e85bd2af020160eced89f9fdff454ebb52b3d37cf35d5f92129f413abcf397dc5147d551b72d298e851bde58b6de11e710e04a15aba9704c97180bf6d72d3db32b2173b7540203a06e1f573f6ba740e501b8c5832139ee1211335115d9ecfbcfda081ceeee24fe230a9140555d5ed248ec07897d99b7f9b34c81c4428f20af7f869bce8b3d5b6fa4109ca030e3ffc620f856e46801bffeda33438f9144f3618771ed21d74d9cb39ca9f71b44c5184b39744c71117f2155f1cc080da10ed1391f44a4d817a21d0d23a9a806213f9ab8d4af16100e19a4122014a88ba322dd7cabaf53a15951f9997be17e27595ad0036cbd1745bc851a2b1b28e75c25ec03146d65430e25fcf07f7c803aa3ebabf8038cc6e122b10b0557d08d62eb06b2adb64512757b0151a279fe4c0b806bd7892064296ceac856da0a7ad58c71ceb6d2bdde194677d2f258fb83c19d2cdefa1522a603e7b2f9fcff8a9992e70f4b32603e26343931bd1795f66b130c44f0bf1395d771f5c90a06c3e63470be60bbbba41b4f4db9ac3489278dc0812e395dbe61b1e48b1a77cc8660f9f9fea2ac6313fe8c8d158dc6cb7e15ced62b980d9af9cfaceaff01f52557d36768c3baa2e8754d9a69e28d095e5b3031652e2ad8def8b4647b514b15c1497dba870001ac2ef87a62026c690040dc523a7326de4813a39e4b7f05b9fd7421e81029b41f2c48a04097a8c2f7dcf974140d3059ccfc751d69f0f82ba512e0a7d926a88f331a4ea03ae3c1512cd5f3baea8c578249b5164f362d7b540f8a6edf58bd06db2f2a00fcfa779458ade4a1ffc2f0c410ad01522c4b623747d6dd3d5f78be624f6915b7c598fe72473f19a5205cd30ecdf82ec8df5312b4e6a969bf14138c5131b40968af6f5f22d17731b6425397ba4d22494960ddbf97debc1c1823eb137452d80b7bdd307add15d3fe2a80ce6c2eaeb1193cef365ec6a34c33e28834705152befddc3613d0179a6995b2313927d5a179264bcb8a51935d3baa0e78648b60c9652e2a5b23cf268f9718c9f3798000bb4361a0e86299e57a5d3e4de71c3a7e4ce305e58eb29544544b71c2bf11c479e8b1fa9fa114c336f440830b3b9da91012c9350b9e935e945c12f8396ef286690ae963e026cb40e15d7dce57c41977d245aca38a23bb6dd9cce8456a1ede6c9b7cf8efaf1e721593f1f327f28b72f2a8ef5fdd488a93bdb36914bf24b9324c0f16efdeb1b2864ec30bc1a4f6d6acd84c38c01e4063dd10ba3b3f181de1a451a5bf35a53b6b01ebe9f9ecebd346ab8533167d5300f6730adeff6647be10571c9f4de3d3a938030ab0f7144dc27b28541af1f2ce5487346c80cef097d8747debd50830dad1e1efa948ddf840f9c3a4b787ef51f9e4cef6d957fb16cd81ed1bcaa0016d350eb34b139c2608d630d75247371bbe9deda4dd18363891e62ecc4785d1b1bab79239bce655f56c02f4b57e5b397e227d9742bbcfb5299ddd407ecd01c8cb09200aa54a7731b97ea61c286dabc38c727947457eb96209348661d48a83509de7fecb64795b8f406966e020525a4f0a506d26a9dd558ed97f21bf0bf1131339ef09837d28013c7d85cc915d508bcfd027324877e9bb447ef5fb1fdc10259c6cc5f84be93871d0f4a07f35199362b013fa1b1f6187bf5cd1173882e2667e76169ae6afb97dc441f50bbac65b3722cfeec55caf31a3e31b02fa60c66caa44c3ecba3673b1cba7d7525ed46979e1929a212bd50819bfaa7f441b527e9708b3f1ac5560626b4b1160acb26c91fb708bad8a0278c11b61963d4ec3d08681c74c7847da8c8998db76bdde6c07e777640c9a6f99cb0ef63bd022c6714a7d6b40353482686d3b63ee3bd5928f1d04ad21b2820e432d37466aad9a86712f66c5fc838ff55680302911ea6a3d85a8b237ab1da09f11e90fd2dff203249300e1f1fede59c3ac2a856adf35ebdc0938fbb4dc3abb2104ec6d8f42657851052dad56af5366b23b6945eb0b18cd85a19caf1acee222aa4ef2bbf913b40fc4447dae412137e55730091c123e797682a9daa1216c337cae0d0f85dc15d79fd91925971ef36f666f94163247a0065ddf1ac5ab12b39b0ff8494511b51a923c97fe1ffb0f3b2a5ada9be2560b417396ad391f1ff366ba1c659022bcbaf76f1bd1a254a13ccb1b706ec930f401f90f9bd70b088810935a24463467ccfb84932794e71603407da863fdec02878cba186f69db56872df722873dc5b8f253e4df46e6dd77cc548f0d6f8bd0878d9e6e5a0eb6bf7376e859bebd59b9ea5d7b9faf40de50cb697e035ddec48c0d1c3c773eb8d04c06b2d94c0983520d4080e54954dd922db6acbca14d7f4d937b8a3314b94bc8e20756f7bfa9ee67b7ea581119cdc8b3d38b73d7d726b610f18c923c7a1fce65ac2549ce93790618efd1c3d7ecc5e6593cb8be533395475419f15dc6abac7e6acfa7739ed9295d65d3e9ad3a3c6c4397797cb7367e450acce37412d40713a36a44a7bd0860f5d9b7e6939abe182eca5805c62b9396eb3d6ababfb9f4f977fdd7edf5184f12a928dff0939a085b21024f1817bf631991f4caaadfef4bceb89bf184aa5f38d5ad86531fbc949e277027d685a07dba09cfb16462994d99ca051308b235425b3b5d2387fd8f58b99872629a01eb07415fe47ae65a353ea990349c057c1498f359069549e4999af2ed0a3dbb5451bc4a616e5a65c9de76ebd5e90d51916e8acb1eaa2af7571ecae2a2066b42f5dc7982140ae1637067163442cd865a6a34227ec838fd446c3c07ec9d1d3fb3430c47314ba53985a9ac513a8d4c6204f841c7742b29db277db3467696e5dc46e67afce99ca3d5afdd66a650eae8e9ea51e65ad32a5d0ec25ee923f4ef8d1f796eb4a2eba866aa8c7901db5d72334c11cd58e0841ae3698c5918ec0afa7ac6380ea8a45b1a6d8976ce02e38e32d35c0fbfc32c7767ad1111f2d86ae39998e1b625ce48d4477e3791a05842addf6bdaaa114b4684e97bb0742f80f34613639650778ace10fcfcb2a008664ee93fea7d0b54e80f9b4bee6ac18b2c1ff4cadbc3d59d379a2b1ec22a35dbc767c8f919f7c2699e3225e83d040e1eac8aab465f20a95909031bc3fad5f2b2616b92b94534b95b0185d22da65b56be6c8327457cb9ca7872433d7522c2a5dca1e0e12acb7179328f938b12e5d6fa8265e32b69844a4de265a7295c201606a2d7608f01b4c0961b2a6955090866c194039352d62edfe2feeb391e4ec9c27c546b19c1f537fd6e7bf26e23696bed10042f2f1da51e5b9db50062cd4fa6976c06fd36d6d7cb3a1538dbeddc06a206d917563ed437c38543c060a651f309ab029f3ccd9da078e5262bd5bd61ed8d7264e4f3e41225f29f6ee322c16a55cee1cc364decc38176dcdd67b33315b32ea53058b23a20b0ed76f58f816d48d921756b70f00a1c5c1d637f2ab35f304b66eae0a26e990c400f9b5a770da5b42b08c900f2a4a4952409a8bae19e37f68604ee654d914b0857c08580a56ffbebaace08dfb8d254ecf59479d5a5688d2189c5f1083642c707e2b4f6b6c15c6f925b8b2e2e89d757a121dbbcac46e994a140eba4cb80bf3865917651248318112cb0b455d263ff5f05726746c5844beda5d928f5fb5e052a2cb5a2f2dbf905c5b52a2834b8c1e31d9e52c67767447beff6cf5dcadf274618e0b3f0026fdde88fc12afc52f2a35bd7fdf7ffc2d7062d711f22189e99b1a6b26bb6e8628cd0945b3f9116bd6d993e06b102794b166fac81690745ba9414be2a512d4d45f2b103e835e21c9d5139821458cdc5fc3527b6bc36cb942a1bab91bff75ec75635f7818dd433c3b9b783a2bc6c1de2157261f1d715cf528c322de1028de8951c3e9142bb9ec802607f4559b731e8ac668b4eb51e21a7211e53b9f225780976a6ae73afcd356294572fbddd86aa96bccbe573a56993b831bd3a7a173ac34ec0f803b727cecf5c96566c98a8f5c742ac3bb92d780caf9297d2e35ca2139399097b73cb628682e15baffefa32d691b8ae38590bac1e42db233d417346c41fee29ea7656c56526d9b9ee75d89d1450369eaba14b71c093c4395eb3b5917ca6b8f92f8fed676eabf255ff0d4ea3d8e3f4ece94f9ee85b11b54fbf04d538135706ea323136b429775d1816a0f551afd4edbfa6822b0f1a4a8ce87459ac33291d8cea394369e767c02aeacecd01d7770bae1e927676ccdff76d35409320412a0fe202640bf77f0b3f537aabd2826fdc324eed2df6c79bddf499bb94e941a7e55ebf891b90ff72a99c1f7fddf197e4b3bce887ca97527d5994e86e7bcdf30d46aab45d9b32ca234f3df846fd8d4975355685c5bb8fbd9298f7bbfce6f5a66c232fec466923e0e7d42dd4afffd72254591d6a387850b9d21e57b780ad88908f815d18b4230e029fc6c287b68b46dadec2b8aa5344bd1263fee89b60c6acd1378dfa9ab8a6a7e9aa5f796e75c420e06806c67db315994296cc15544b2fb1f0e438a6eac5b14d73be8dc77528a1f97d94c65768bd340a0859a5d9c8ee070d82f983fa9c3efbd5eb6e8a34fcaa4151be34274a208d5eebfc3a9466075f7157b38387999279e61a7e7a7e5829168cb2506f78faee50ef4ea50d1f6e1b427aee75a4e68cf2cafffe43ae4b90dd5c00721ba25dc53c5c633b500ad4d1d8e804bfaed259174f1108afaa274e7e5d03615fa6fac467ecba2e053fa31c51c97e6882e9d0a6c9727469bed8ab3eceed4d68f156bd673973acde39bf6935a0068880956ea3a3964f0874b283d69c4ab18ec922261dd19d266ee3c2bb0d7297274b7b96b727a85ba4937afac1c62361ea69dddacf1da4e1b1ff77371123924efbeced3851730f2399e4fd8b1b60565879b444b86babdea7d7947183187fbba8a45ab562a8588a143feda42066ca2a44b718d8f</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
    
    
    <categories>
      
      <category>工程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>文档</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>技术链接</title>
    <link href="/2024/03/05/%E6%96%87%E6%A1%A3/%E6%8A%80%E6%9C%AF%E9%93%BE%E6%8E%A5/"/>
    <url>/2024/03/05/%E6%96%87%E6%A1%A3/%E6%8A%80%E6%9C%AF%E9%93%BE%E6%8E%A5/</url>
    
    <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="f3a21b88b9c596ac2a7e3847019e81e359110a62c3b546080d87f667959e8c0b">4c7d53edba4f7233f4dcfda30a30b6935cb6dfb37e53dec01fd4b04ca04aa3b568483c2854573f489c7293d481e8877423130fa94958a0fbbb7438d463bf8edda14f9146b42190a199fe00963f7796dc60df9716f127fdbe836a59b371b22e8fd5338857768dc091f6df09419bbf850aaf8750b480cb44d775735ef5b76a7c38c066665aee6fa5976befbe36fd9b75911a262d65890fb36534f61616ff27c76bb2726bfd7adcad3d32ea6fd4910f9d99a4926f5259a9982eba8666b701cc5c08bc3f05751bd4d1a3e4360f557ddeb92e6fc8c4ac81bdfb9873c4bf6045bf39cf4149f09e04478b60014604837bf2315fd532ce1b337416d5f02d8ced61176ab2477ba30ac0bf35e996d9a96cdceef20031b1904e96f89445758d7d015cba727083c24bd1b85543450f5b9634fdf80a5e1426e65af6ae62965e3b19fac0179cb532132a00b642e5422ef3e978583cf39ecf41bfbfc3b71a08af8e51e80ef22e62ec9a3d93b7aaf51d47e1926595bbf5f35be3a90d0a75a52684a77dfec0af35ecc7b005297fb186f42ed08d63db5cc11d289e2f3286dd79e0c5085eefbbc26bd4</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
    
    
    <categories>
      
      <category>工程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>文档</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>利用深度强化学习避免融合等离子体撕裂不稳定性</title>
    <link href="/2024/03/05/%E4%BA%A4%E5%8F%89/8%E9%81%BF%E5%85%8D%E7%AD%89%E7%A6%BB%E5%AD%90%E4%BD%93%E4%B8%8D%E7%A8%B3%E5%AE%9A%E6%80%A7,Nautre24/"/>
    <url>/2024/03/05/%E4%BA%A4%E5%8F%89/8%E9%81%BF%E5%85%8D%E7%AD%89%E7%A6%BB%E5%AD%90%E4%BD%93%E4%B8%8D%E7%A8%B3%E5%AE%9A%E6%80%A7,Nautre24/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="Avoiding-fusion-plasma-tearing-instability-with-deep-reinforcement-learning"><a href="#Avoiding-fusion-plasma-tearing-instability-with-deep-reinforcement-learning" class="headerlink" title="Avoiding fusion plasma tearing instability with deep reinforcement learning"></a>Avoiding fusion plasma tearing instability with deep reinforcement learning</h1><p>利用深度强化学习避免融合等离子体撕裂不稳定性<br><a href="https://mp.weixin.qq.com/s/MpIVfNJVpZzBncgqll6Y3Q">https://mp.weixin.qq.com/s/MpIVfNJVpZzBncgqll6Y3Q</a><br><a href="https://www.nature.com/articles/s41586-024-07024-9">https://www.nature.com/articles/s41586-024-07024-9</a></p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>有必要根据观察到的等离子体状态主动控制托卡马克，在操纵高压等离子体的同时避免撕裂不稳定，这是导致中断的主要原因。这提出了一个基于强化学习的人工智能最近表现出显着性能的避障问题</p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1709011867860-14b1f6e8-cbcc-4fdd-90c2-1bffb7e3e960.png#averageHue=%23e7e7e7&clientId=u7685dfc0-f033-4&from=paste&height=724&id=u195af0e1&originHeight=1086&originWidth=890&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=142787&status=done&style=none&taskId=ud0e2b34c-a43c-4d86-b58d-f332a9c4355&title=&width=593.3333333333334"><br>提取特征，模态信息融合<br><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1709012031092-a640efe2-fcf5-483e-9aeb-16709b0a72bc.png#averageHue=%23e7e7e7&clientId=u0dd9fd2f-e670-4&from=paste&height=471&id=uf97b6fb1&originHeight=707&originWidth=651&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=97083&status=done&style=none&taskId=u4ad23a0f-9cfc-4b7c-baa7-43c85365fc7&title=&width=434"><br>AC算法进行预测，某个状态的撕裂可能性</p><h1 id="未来工作"><a href="#未来工作" class="headerlink" title="未来工作"></a>未来工作</h1>]]></content>
    
    
    <categories>
      
      <category>科研</category>
      
    </categories>
    
    
    <tags>
      
      <tag>交叉</tag>
      
      <tag>Nature</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MAT</title>
    <link href="/2024/03/05/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/7MAT,NeurIPS22/"/>
    <url>/2024/03/05/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/7MAT,NeurIPS22/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="Multi-agent-reinforcement-learning-is-a-sequence-modeling-problem"><a href="#Multi-agent-reinforcement-learning-is-a-sequence-modeling-problem" class="headerlink" title="Multi-agent reinforcement learning is a sequence modeling problem"></a>Multi-agent reinforcement learning is a sequence modeling problem</h1><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>MAT的核心是一个编码器-解码器架构，该架构利用多智能体优势分解定理将联合策略搜索问题转换为顺序决策过程;这使得多智能体问题的时间复杂度仅为线性</p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="多智能体优势分解理论"><a href="#多智能体优势分解理论" class="headerlink" title="多智能体优势分解理论"></a>多智能体优势分解理论</h2><p><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1709013797645-8b43fdd5-970f-4401-868b-da858f47c012.png#averageHue=%23fbf9f8&clientId=u2e1b7fe2-aff5-4&from=paste&id=ud4231016&originHeight=160&originWidth=1048&originalType=url&ratio=1.5&rotation=0&showTitle=false&size=45056&status=done&style=none&taskId=u9aeeaf89-82a5-4748-b506-b7029ca2ccf&title="><br>目标的联合性导致了与信用分配问题相关的困难，在获得共享奖励后，个体智能体无法推断自己对团队成功或失败的贡献<br>定义多<strong>智能体观测值函数</strong><br><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1709014024176-fbd2658e-c540-44dc-95f5-063c97fbe8aa.png#averageHue=%23f9f8f6&clientId=uf84c472e-ae7a-4&from=paste&height=69&id=u2a147843&originHeight=48&originWidth=402&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=10974&status=done&style=none&taskId=ud43c93f2-4f73-4c91-b58b-5f4830c49a6&title=&width=580"><br>衡量所选智能体子集对联合回报的贡献<br><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1709014167414-4d2603f6-ddfc-4d0f-877e-982b1b3b9e8e.png#averageHue=%23faf8f6&clientId=u9597bc06-3114-4&from=paste&height=66&id=uecf2a4c1&originHeight=48&originWidth=560&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=8523&status=done&style=none&taskId=u58a340ef-329f-4813-881a-3f1610243f1&title=&width=764.3333435058594"></p><h2 id="多智能体优势分解定理"><a href="#多智能体优势分解定理" class="headerlink" title="多智能体优势分解定理"></a>多智能体优势分解定理</h2><p><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1709014340231-73d07de4-2a2a-4d41-b79d-743ee9d099fe.png#averageHue=%23f4f0eb&clientId=u9597bc06-3114-4&from=paste&height=377&id=u97b242b3&originHeight=411&originWidth=811&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=163227&status=done&style=none&taskId=uf2e6a335-edef-4cd2-b154-d4f233be42b&title=&width=744.6666870117188"><br>假设智能体1选择一个正向的行动，智能体2-n都知道智能体1选择了一个正向行动<br>所有智能体分别搜索一个正向的行动进行选择，构成Transformer序列模型<br><strong>后续智能体决策将依赖于前序智能体决策</strong></p><h2 id="用Transformer来解决强化学习问题"><a href="#用Transformer来解决强化学习问题" class="headerlink" title="用Transformer来解决强化学习问题"></a>用Transformer来解决强化学习问题</h2><p><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1709014528775-8bf8ab3c-74f5-436d-a913-981dd00a1a2c.png#averageHue=%23f2efe9&clientId=u9597bc06-3114-4&from=paste&height=388&id=ud2dd568a&originHeight=582&originWidth=1138&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=390490&status=done&style=none&taskId=u3828f17a-1a23-496e-868e-a13db96a01f&title=&width=758.6666666666666"><br>编码器<br><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1709014654180-d20e6817-7c9d-4962-b827-591d41608e80.png#averageHue=%23faf9f7&clientId=u9597bc06-3114-4&from=paste&height=77&id=ufec8b6ea&originHeight=116&originWidth=769&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=18569&status=done&style=none&taskId=uc206188d-fad3-4bce-83b2-949bf993669&title=&width=512.6666666666666"><br>解码器<br><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1709014665741-622a8d25-7ca6-406c-adcb-0f420997e7aa.png#averageHue=%23fbf9f8&clientId=u9597bc06-3114-4&from=paste&height=151&id=u63ba0caf&originHeight=226&originWidth=803&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=36442&status=done&style=none&taskId=uf67e428a-f04b-4b76-a5d1-4ba65df5c60&title=&width=535.3333333333334"></p><h1 id="未来工作"><a href="#未来工作" class="headerlink" title="未来工作"></a>未来工作</h1><p>在未来，我们计划将多智能体学习任务引入大型多模态SM中，追求更普遍的智能模型，正如GATO最近的成功已经证明的那样</p>]]></content>
    
    
    <categories>
      
      <category>科研</category>
      
    </categories>
    
    
    <tags>
      
      <tag>经典算法</tag>
      
      <tag>NeurIPS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>QMIX</title>
    <link href="/2024/03/05/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/6QMIX,ICML18/"/>
    <url>/2024/03/05/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/6QMIX,ICML18/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="Qmix-Monotonic-value-function-factorisation-for-deep-multi-agent-reinforcement-learning"><a href="#Qmix-Monotonic-value-function-factorisation-for-deep-multi-agent-reinforcement-learning" class="headerlink" title="Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning"></a>Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning</h1><p>Qmix： 用于深度多代理强化学习的单调值函数因式分解<br><a href="https://zhuanlan.zhihu.com/p/353524210">https://zhuanlan.zhihu.com/p/353524210</a><br><a href="https://proceedings.mlr.press/v80/rashid18a/rashid18a.pdf">https://proceedings.mlr.press/v80/rashid18a/rashid18a.pdf</a></p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>多智能体强化学习训练中面临的最大问题是：训练阶段和执行阶段获取的信息可能存在不对等问题。即，在训练的时候我们可以获得<strong>大量的全局信息</strong>。<br>但在最终应用模型的时候，我们是无法获取到训练时那么多的全局信息的，因此，人们提出两个训练网络：一个为<strong>中心式训练网络</strong>，该网络只在训练阶段存在，获取全局信息作为输入并指导智能体行为控制网络进行更新；另一个为<strong>行为控制网络</strong>，该网络也是最终被应用的网络，在训练和应用阶段都保持着相同的数据输入。</p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1709011958605-9f6be777-4d3d-4b03-a431-213b32be32b6.png#averageHue=%23faf9f9&clientId=u37125000-3bca-4&from=paste&height=297&id=uda8c91dd&originHeight=446&originWidth=1049&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=217253&status=done&style=none&taskId=ucfe49fe2-9133-4813-9456-cde6c504f57&title=&width=699.3333333333334"><br> Mixing Network（类比 Critic 网络）和 Agent RNN Network（类比 Actor 网络）</p><h2 id="Agent-RNN-Network"><a href="#Agent-RNN-Network" class="headerlink" title="Agent RNN Network"></a>Agent RNN Network</h2><p>QMIX 中每一个 Agent 都由 RNN 网络控制，训练时可以为每一个 Agent 个体都训练一个独立的 RNN 网络，同样也可以所有 Agent 复用同一个 RNN 网络<br><strong>MLP+GRU+MLP</strong></p><h2 id="Mixing-Network"><a href="#Mixing-Network" class="headerlink" title="Mixing Network"></a>Mixing Network</h2><p>网络同时接受Agent RNN Network的Q值和全局状态<br><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1709016512747-ea8e8282-b8b4-47d5-9a0c-ef23156981f7.png#averageHue=%23f7f6f5&clientId=u4bf42cf8-5ba7-4&from=paste&id=ufa3733c7&originHeight=315&originWidth=804&originalType=url&ratio=1.5&rotation=0&showTitle=false&size=32258&status=done&style=none&taskId=u225a8851-1523-4ecf-bd61-4fd1345fc72&title="><br>训练过程中，使用全局信息生成参数，然后得到总体函数Qtot<br>实际使用过程中，只使用推理网络（蓝色部分）<br><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1709016666104-f0894631-7155-4d7b-a344-5346ac1a976f.png#averageHue=%23f9f0ef&clientId=u4bf42cf8-5ba7-4&from=paste&height=276&id=ucb90c6d8&originHeight=414&originWidth=296&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=73839&status=done&style=none&taskId=u9d2a8d25-3196-4bfb-b15c-d20c2cb255c&title=&width=197.33333333333334"></p><h1 id="未来工作"><a href="#未来工作" class="headerlink" title="未来工作"></a>未来工作</h1><p>在不久的将来，我们的目标是进行更多的实验，以比较具有更多数量和更大多样性的单元的任务方法。从长远来看，我们的目标是为具有许多学习代理的设置提供更协调的探索方案来补充QMIX。</p>]]></content>
    
    
    <categories>
      
      <category>科研</category>
      
    </categories>
    
    
    <tags>
      
      <tag>经典算法</tag>
      
      <tag>ICML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VDN</title>
    <link href="/2024/03/05/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/5VDN,AAMAS18/"/>
    <url>/2024/03/05/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/5VDN,AAMAS18/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="Value-decomposition-networks-for-cooperative-multi-agent-learning-based-on-team-reward"><a href="#Value-decomposition-networks-for-cooperative-multi-agent-learning-based-on-team-reward" class="headerlink" title="Value-decomposition networks for cooperative multi-agent learning based on team reward"></a>Value-decomposition networks for cooperative multi-agent learning based on team reward</h1><p>基于团队奖励的多机器人合作学习价值分解网络<br><a href="https://arxiv.org/pdf/1706.05296.pdf">https://arxiv.org/pdf/1706.05296.pdf</a><br><a href="https://zhuanlan.zhihu.com/p/362191316">https://zhuanlan.zhihu.com/p/362191316</a></p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>由于每个智能体都是局部观测，那么对其中一个智能体来说，其获得的团队奖励很有可能是其队友的行为导致的。也就是说该奖励值对该智能体来说，是<strong>虚假奖励</strong>。因此，每个智能体独立使用强化学习算法学习 往往效果很差。<br>这种虚假奖励还会伴随一种现象，作者称作<strong>惰性智能体</strong>。当团队中的部分智能体学习到了比较好的策略并且能够完成任务时，其它智能体不需要做什么也能获得不错的团队奖励</p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1709014940404-fad5c85e-30c7-496e-bab2-329ecdab5741.png#averageHue=%23f0f0f0&clientId=u549eeb9d-cb66-4&from=paste&height=445&id=u6b271d40&originHeight=328&originWidth=246&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=15484&status=done&style=none&taskId=u593b7e94-dc78-4514-ac75-f10df4bf304&title=&width=334"><br>对整体的Q函数值进行分解，在智能体综合优化最大时，得到每个智能体的具体情况<br>每个<strong>智能体局部</strong>通过贪心算法来选择动作，综合起来是整个智能体的得分<br><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1709015779181-1a60b4b0-98a4-4340-87c3-5662c7562c31.png#averageHue=%23faf9f7&clientId=u047a06e4-10ec-4&from=paste&height=48&id=u3efb2386&originHeight=51&originWidth=533&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=16657&status=done&style=none&taskId=u74805035-e8f6-4f1c-b5a5-87143be6cdb&title=&width=497.3333435058594"></p><h2 id="未来工作"><a href="#未来工作" class="headerlink" title="未来工作"></a>未来工作</h2><p>在未来的工作中，我们将研究随着团队规模的增长价值分解的尺度，这使得拥有团队奖励的个体学习者更加困惑(他们大多看到来自其他代理行为的奖励)，而集中式学习者甚至更加不切实际。我们还将研究基于非线性值聚合的分解。</p>]]></content>
    
    
    <categories>
      
      <category>科研</category>
      
    </categories>
    
    
    <tags>
      
      <tag>经典算法</tag>
      
      <tag>AAMAS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MAPPO</title>
    <link href="/2024/03/05/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/4MAPPO,NeurIPS22/"/>
    <url>/2024/03/05/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/4MAPPO,NeurIPS22/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="The-surprising-effectiveness-of-PPO-in-cooperative-multi-agent-games"><a href="#The-surprising-effectiveness-of-PPO-in-cooperative-multi-agent-games" class="headerlink" title="The surprising effectiveness of PPO in cooperative multi-agent games"></a>The surprising effectiveness of PPO in cooperative multi-agent games</h1><p>PPO 在多代理合作博弈中的惊人效力<br><a href="https://arxiv.org/pdf/2103.01955.pdf">https://arxiv.org/pdf/2103.01955.pdf</a><br><a href="https://zhuanlan.zhihu.com/p/386559032">https://zhuanlan.zhihu.com/p/386559032</a><br><a href="https://github.com/marlbenchmark/on-policy">https://github.com/marlbenchmark/on-policy</a></p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>近端策略优化算法的使用率较低，验证了PPO算法的有效，整体感觉是工程上的Trick，就是把PPO算法移植到多智能体上了</p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1709012631657-a78aeba1-1c1b-4729-a9dd-58558334ee89.png#averageHue=%23f9f8f6&clientId=u174c384c-a19d-4&from=paste&height=567&id=u38aa7e7b&originHeight=851&originWidth=860&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=186307&status=done&style=none&taskId=u99b3ddb7-6d87-43ff-9ad7-62e663300fa&title=&width=573.3333333333334"><br>Actor优化目标<br><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1709012488891-f7bf2add-06a6-4729-9218-56b2eea347e0.png#averageHue=%23fefefe&clientId=u174c384c-a19d-4&from=paste&height=169&id=ud0036227&originHeight=253&originWidth=1145&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=57738&status=done&style=none&taskId=u6d27f52a-2d18-4ee3-9c54-40422058373&title=&width=763.3333333333334"><br>Critic优化目标<br><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1709012504862-9aa6c50d-cf15-4392-88d2-1c2604effbe2.png#averageHue=%23fefefe&clientId=u174c384c-a19d-4&from=paste&height=153&id=u5d342d6c&originHeight=229&originWidth=984&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=45671&status=done&style=none&taskId=u41d80b20-6717-4cdb-bfab-a471e54e7a9&title=&width=656"><br>所有的Actor，Critic都用<strong>全局的信息</strong>进行优化</p><h1 id="未来工作"><a href="#未来工作" class="headerlink" title="未来工作"></a>未来工作</h1><p>首先，我们的基准环境都使用离散的动作空间，都是合作的，并且在绝大多数情况下，包含同质代理。在未来的工作中，我们的目标是在更广泛的领域上测试PPO，例如具有连续动作空间和异构代理的竞争性游戏和MARL问题。此外，我们的工作主要是经验性的，并没有直接分析PPO的理论基础。我们相信对我们的建议的实证分析可以作为进一步分析PPO在MARL中的特性的起点。</p>]]></content>
    
    
    <categories>
      
      <category>科研</category>
      
    </categories>
    
    
    <tags>
      
      <tag>经典算法</tag>
      
      <tag>NeurIPS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MADDPG</title>
    <link href="/2024/03/05/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/3MADDPG,NeurIPS17/"/>
    <url>/2024/03/05/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/3MADDPG,NeurIPS17/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="Multi-Agent-Actor-Critic-for-Mixed-Cooperative-Competitive-Environments"><a href="#Multi-Agent-Actor-Critic-for-Mixed-Cooperative-Competitive-Environments" class="headerlink" title="Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments"></a>Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments</h1><p>混合合作竞争环境中的多代理行动者评判器<br><a href="https://arxiv.org/pdf/1706.02275.pdf">https://arxiv.org/pdf/1706.02275.pdf</a><br><a href="https://zhuanlan.zhihu.com/p/53811876">https://zhuanlan.zhihu.com/p/53811876</a></p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>q学习（基于值学习）受到环境固有的非平稳性的挑战，而策略梯度则受到随着智能体数量增加而增加的方差的影响，主要对AC算法做了改进，关注多智能体问题<br>MADDPG算法具有以下三点特征： 1. 通过学习得到的最优策略，在应用时只利用局部信息就能给出最优动作。 2. 不需要知道环境的动力学模型以及特殊的通信需求。 3. 该算法不仅能用于合作环境，也能用于竞争环境</p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1709009798947-5d401739-f17d-47c0-aa14-708f8c18d9d4.png#averageHue=%23faefed&clientId=uceaab4db-45f6-4&from=paste&height=450&id=udb4dc883&originHeight=258&originWidth=357&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=17299&status=done&style=none&taskId=ueb80efb2-2438-4f25-899b-13d6ca5f9a0&title=&width=622.6666870117188"><br>在以下约束下运行:(1)学习到的策略在执行时只能使用局部信息(即它们自己的观察结果)，(2)我们不假设环境动态的可微分模型，这与中不同，(3)我们不假设代理之间的通信方法有任何特定的结构(即，我们不假设通信通道可微分)。</p><h2 id="集中训练分散执行"><a href="#集中训练分散执行" class="headerlink" title="集中训练分散执行"></a>集中训练分散执行</h2><p><strong>Critic扩展为可以利用其他智能体的策略进行学习</strong><br>集中式训练，分布式执行：训练时采用集中式学习训练critic与actor，使用时actor只用知道局部信息就能运行。critic需要其他智能体的策略信息，本文给了一种估计其他智能体策略的方法，能够只用知道其他智能体的观测与动作。</p><h2 id="对其他智能体策略进行估计"><a href="#对其他智能体策略进行估计" class="headerlink" title="对其他智能体策略进行估计"></a>对其他智能体策略进行估计</h2><p><strong>每个智能体额外优化n-1个逼近函数</strong><br>通过对其他智能体的策略进行估计来实现。每个智能体维护n-1个策略逼近函数，降低了通信的消耗<img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1709010712353-9e8e4280-2e50-4dd4-bcc5-4e1725a8b55c.png#averageHue=%23f0eeec&clientId=u0fa03937-0d35-4&from=paste&height=523&id=u78d6d771&originHeight=784&originWidth=1640&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=324644&status=done&style=none&taskId=u67f8595b-aa92-406c-a96b-fab6d0fb124&title=&width=1093.3333333333333"></p><h2 id="具有策略集合的代理"><a href="#具有策略集合的代理" class="headerlink" title="具有策略集合的代理"></a>具有策略集合的代理</h2><p><strong>每个智能体对整体的贡献进行优化，使用一个整体的优化合集</strong><br><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1709010898813-270cf31a-773c-43ad-80dd-1283aa5c1bf3.png#averageHue=%23faf9f8&clientId=u0fa03937-0d35-4&from=paste&height=98&id=u116798af&originHeight=147&originWidth=1415&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=35140&status=done&style=none&taskId=u6658c6a7-9e45-4e9b-a6e3-e762e8ef55e&title=&width=943.3333333333334"></p><h1 id="未来工作"><a href="#未来工作" class="headerlink" title="未来工作"></a>未来工作</h1><p>Q的输入空间随着代理n的数量线性增长(取决于x中包含的信息)。这可以在实践中得到补救，例如，使用一个模块化的Q函数，它只考虑给定代理的某个邻域内的代理。我们把这项调查留给未来的工作。</p>]]></content>
    
    
    <categories>
      
      <category>科研</category>
      
    </categories>
    
    
    <tags>
      
      <tag>经典算法</tag>
      
      <tag>NeurIPS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>开放环境下的协作多智能体强化学习进展综述</title>
    <link href="/2024/03/04/%E7%BB%BC%E8%BF%B0/2%E5%BC%80%E6%94%BE%E7%8E%AF%E5%A2%83%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0,%E4%BF%A1%E6%81%AF%E7%A7%91%E5%AD%A623/"/>
    <url>/2024/03/04/%E7%BB%BC%E8%BF%B0/2%E5%BC%80%E6%94%BE%E7%8E%AF%E5%A2%83%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0,%E4%BF%A1%E6%81%AF%E7%A7%91%E5%AD%A623/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="开放环境下的协作多智能体强化学习进展综述"><a href="#开放环境下的协作多智能体强化学习进展综述" class="headerlink" title="开放环境下的协作多智能体强化学习进展综述"></a>开放环境下的协作多智能体强化学习进展综述</h1><p>论文地址：<a href="https://arxiv.org/pdf/2312.01058.pdf">https://arxiv.org/pdf/2312.01058.pdf</a>，<a href="https://www.lamda.nju.edu.cn/lilh/file/openmarl.pdf">https://www.lamda.nju.edu.cn/lilh/file/openmarl.pdf</a></p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>1.协作多智能体强化学习专注于训练<strong>智能体团队</strong>以协同完成单智能体难以应对的任务目标<br>2. 以往的研究工作主要 在<strong>简单、静态和封闭</strong>的环境设定中展开。随着人工智能技术落地的驱使，目前在多智能体协作领域 也有部分研究开始对<strong>开放环境</strong>下的多智能体协作展开研究，这些工作从多个方面对智能体所处环境 中要素可能发生改变这一情况进行探索与研究，并取得一定进展。<br>3. 挑战：真实多智能体系统所处环境往往是<strong>部分可观测</strong>的，单个智能体无法从其局部观测中获得环境的全局信息 ； 由于其他智能体同时进行学习，策略相应地会发生变化，从单个智能体角度来看，其处于一个<strong>非稳态</strong>的环境中，收敛性无法得到保证；协作型多智能体系统往往只能得到共享奖赏，<strong>如何将其分配</strong>从而为每个智能体提供准确的反馈。<br>4 .协作型多智能体强化学习都展 现出比传统方法更优秀的性能。研究者设计出许多方法以促进智能体之间的协作，包括基于策略梯 度的方法，如MADDPG[<a href="https://www.yuque.com/xilou-f7acw/paper2024/ppm36nxcafk1sqry">https://www.yuque.com/xilou-f7acw/paper2024/ppm36nxcafk1sqry</a>]和MAPPO<a href="https://www.yuque.com/xilou-f7acw/paper2024/wn5oi47s59nhym6t">https://www.yuque.com/xilou-f7acw/paper2024/wn5oi47s59nhym6t</a>]；基于值函数的方法，如VDN[<a href="https://www.yuque.com/xilou-f7acw/paper2024/eb398k8avyykb064">https://www.yuque.com/xilou-f7acw/paper2024/eb398k8avyykb064</a>]和QMIX[<a href="https://www.yuque.com/xilou-f7acw/paper2024/rgzsd619w53g4yn7">https://www.yuque.com/xilou-f7acw/paper2024/rgzsd619w53g4yn7</a>]；或包 括借助Transformer 的强大表达能力提升协作能力的 MAT [<a href="https://www.yuque.com/xilou-f7acw/paper2024/uz00n69gcisfokin">https://www.yuque.com/xilou-f7acw/paper2024/uz00n69gcisfokin</a>] 在内的其他方法。<br>5. 开放环境下的任务，主要工作包括可信强化学习、环境生成与策略学习、持续强化学习、强化学习泛化能力 、元强化学习与模拟器到真实环境的策略迁移 等。  </p><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p><strong>强化学习</strong>旨在指导智能体依据当前的状态学习执行恰当的动作，即学习到一个观测状态到动作的映射，通过决策以最大化环境反馈的累积数值奖赏（reward）。环境会依据当前状态和智能体 执行的动作反馈其相应的奖赏信息。智能体无法得知要执行的最优动作，而必须通过尝试从而发现 那些能产生最大累积奖赏的动作。在标准RL场景中，智能体通过观测状态和执行动作与环境交互。 在交互的每个时间步，智能体都会接收对环境当前状态的观测，然后依据该观测选择动作作为输出。 上述动作的执行会改变环境的状态，并发送给智能体环境反馈的奖赏信号。智能体的目的是执行能 最大化累积数值奖赏的动作序列。</p><h2 id="四个部分"><a href="#四个部分" class="headerlink" title="四个部分"></a>四个部分</h2><p><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1708486411342-7ad6d107-6df3-4fba-a8bb-7c998dbc8b50.png#averageHue=%23f6f6f6&clientId=u34a8521f-39fb-4&from=paste&height=360&id=uaf80ab26&originHeight=540&originWidth=853&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=62320&status=done&style=none&taskId=u8274000d-a1eb-4a77-a4a4-950a70ec528&title=&width=568.6666666666666"><br>智能体（Agent）、状态（State）、动作（Action）和奖赏 （Reward）  </p><h2 id="马尔可夫决策过程"><a href="#马尔可夫决策过程" class="headerlink" title="马尔可夫决策过程"></a>马尔可夫决策过程</h2><p><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1708486772589-ac4629ae-102c-469b-a381-025fa6675116.png#averageHue=%23f3f3f3&clientId=u0c974a61-2d5f-4&from=paste&height=215&id=uc9784dbe&originHeight=322&originWidth=1254&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=81893&status=done&style=none&taskId=uadb0ff74-1c42-4d13-812d-1e0356d6a69&title=&width=836"><br><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1708487168375-aded07dc-7e8a-4ba3-846f-4a0e190975d1.png#averageHue=%23f0f0f0&clientId=u0c974a61-2d5f-4&from=paste&height=32&id=uc2e16b2f&originHeight=48&originWidth=284&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=3259&status=done&style=none&taskId=u24440a09-0c9f-48d6-870d-a6be19bb05a&title=&width=189.33333333333334"></p><ul><li>S × A × S 表示函数的输入，即当前状态、当前动作和下一个状态的组合。</li><li>[0, 1] 是输出值的范围，表示概率值，因为概率总是介于0（不可能发生）和1（必然发生）之间。</li></ul><p><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1708486798409-8d813f14-cd1f-4e8e-877c-3b1f54bd25a9.png#averageHue=%23f5f5f5&clientId=u0c974a61-2d5f-4&from=paste&height=41&id=ub7c327d8&originHeight=61&originWidth=577&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=6148&status=done&style=none&taskId=ufc79ca23-a394-4a58-8a7e-907bfd6f8c7&title=&width=384.6666666666667">s状态a行动，转移到s’的概率（类似马尔可夫链）<br><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1708487095942-abfbcde9-4ab0-4b76-ba2e-35d9af5d4bba.png#averageHue=%23f1f1f1&clientId=u0c974a61-2d5f-4&from=paste&height=32&id=u6ede432d&originHeight=48&originWidth=396&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=4782&status=done&style=none&taskId=u661e42d0-1410-42de-99ea-921b4ebb3fd&title=&width=264">s状态a行动得到的奖赏分数，为什么是期望值：有波动，多次统计等</p><h3 id="折扣因子（γ）"><a href="#折扣因子（γ）" class="headerlink" title="折扣因子（γ）"></a>折扣因子（γ）</h3><ul><li><strong>定义</strong>：折扣因子是一个介于0和1之间的参数，用于调整未来奖赏的当前价值。它反映了智能体对未来奖赏的重视程度。当γ接近1时，智能体更倾向于考虑长远的奖赏；当γ接近0时，智能体则更关注即时的奖赏。</li><li><strong>作用</strong>：折扣因子允许智能体在决策时权衡即时奖赏和未来可能获得的奖赏。在实际应用中，由于资源和时间的限制，智能体往往无法等待无限长的时间来收集奖赏，因此需要通过折扣因子来平衡即时和未来奖赏。</li></ul><h3 id="最大轨迹长度（T）"><a href="#最大轨迹长度（T）" class="headerlink" title="最大轨迹长度（T）"></a>最大轨迹长度（T）</h3><ul><li><strong>定义</strong>：最大轨迹长度是指智能体与环境交互过程中，状态转移序列的最大长度。在强化学习中，轨迹（Trajectory）是从初始状态开始，通过一系列状态转移和奖赏，直到达到终止状态的完整路径。</li><li><strong>作用</strong>：最大轨迹长度为智能体的学习过程设定了一个时间限制。在实际应用中，智能体可能需要在有限的时间内完成任务，或者在资源有限的情况下进行学习。通过设定最大轨迹长度，可以确保智能体在有限的交互次数内学习到有效的策略</li></ul><h3 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h3><p><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1708487367709-bc206d9d-f07c-43b8-a39d-a0aa6d427907.png#averageHue=%23f6f6f6&clientId=u0c974a61-2d5f-4&from=paste&height=86&id=u53c0768e&originHeight=129&originWidth=324&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=6985&status=done&style=none&taskId=u36b14efa-128c-4b8a-bf44-d3b89c03332&title=&width=216"><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1708487973534-21a74bfd-4129-4372-8994-3e923b835449.png#averageHue=%23f1f1f1&clientId=u0c974a61-2d5f-4&from=paste&height=314&id=uf4647f0e&originHeight=471&originWidth=1288&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=151022&status=done&style=none&taskId=ue3f52a1d-48a7-470c-84af-cf4507456d1&title=&width=858.6666666666666"></p><h3 id="基于值函数的强化学习-VB，q-learning"><a href="#基于值函数的强化学习-VB，q-learning" class="headerlink" title="基于值函数的强化学习(VB，q-learning)"></a>基于值函数的强化学习(VB，q-learning)</h3><p>类似于贪心算法，是否选择某个状态s下的a动作，使得Q值变大<br><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1708498982473-2d7dc08f-839b-49bd-842b-ed7ef7d95a4a.png#averageHue=%23f9f7f5&clientId=u3dbbdc81-488a-4&from=paste&height=438&id=u5272b94f&originHeight=416&originWidth=922&originalType=binary&ratio=0.949999988079071&rotation=0&showTitle=false&size=156775&status=done&style=none&taskId=u9b46e363-59ca-4ba1-b920-0a11153e7a8&title=&width=970.5263279679741"><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1708499588006-c2585434-180f-4149-b41d-76037d0f8af9.png#averageHue=%23f8f5f1&clientId=ud5db8f74-d884-4&from=paste&height=28&id=W875O&originHeight=27&originWidth=94&originalType=binary&ratio=0.949999988079071&rotation=0&showTitle=false&size=1936&status=done&style=none&taskId=ue6bbbe11-a3c7-4b71-b8ad-1fd221b0ed8&title=&width=98.94736966267848"><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1708499636439-ba0c111f-4230-445c-b88f-4e8b069e7393.png#averageHue=%23f9f6f3&clientId=ud5db8f74-d884-4&from=paste&height=27&id=uf593f62f&originHeight=35&originWidth=225&originalType=binary&ratio=0.949999988079071&rotation=0&showTitle=false&size=4539&status=done&style=none&taskId=ua61b01b9-f116-49ef-9b8b-9cfdbcbb4dc&title=&width=174.82237243652344"><strong>指示函数</strong>决定是否进行这个行为<br><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1708499697879-88865f02-0a2c-49cf-8331-8faea01d7aca.png#averageHue=%23fbf9f7&clientId=ud5db8f74-d884-4&from=paste&height=185&id=u34a44668&originHeight=176&originWidth=546&originalType=binary&ratio=0.949999988079071&rotation=0&showTitle=false&size=63069&status=done&style=none&taskId=udb4b01f0-3329-4dcf-867b-ce91c8988ac&title=&width=574.7368493172602"></p><h3 id="基于策略梯度的强化学习-PG"><a href="#基于策略梯度的强化学习-PG" class="headerlink" title="基于策略梯度的强化学习(PG)"></a>基于策略梯度的强化学习(PG)</h3><p><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1709008509941-98fcffbc-9dbd-4930-93d0-8594b4481850.png#averageHue=%23f4f3f3&clientId=u6ddca154-7380-4&from=paste&height=417&id=u358ee8c5&originHeight=626&originWidth=964&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=90412&status=done&style=none&taskId=u3cbf9054-64d0-4390-b47b-c1d2c299a7d&title=&width=642.6666666666666"><br>对最优策略进行优化，使用梯度方法</p><h3 id="基于行动者-评论者-AC"><a href="#基于行动者-评论者-AC" class="headerlink" title="基于行动者-评论者(AC)"></a>基于行动者-评论者(AC)</h3><p>传统的行动者-评论者（Actor-Critic）方法由两个部分组成：Actor，用以调整策略πθ 的参数θ； 以及Critic，用来调整状态-动作值函数Qπθ w 的参数w。<br><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1709009002718-0f56b8f7-a7b1-4157-a38d-f0455e9f6422.png#averageHue=%23f4f4f4&clientId=u6ddca154-7380-4&from=paste&height=82&id=u62561d60&originHeight=123&originWidth=773&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=15264&status=done&style=none&taskId=uafcd18bb-814e-4a96-82c6-5db360b667d&title=&width=515.3333333333334"><br>Actor-Critic 方法将策略梯度方法和值函数近似的方法两者结合，Actor基于概率选择动作，Critic 基于 Actor 选择的动作以及当前状态评判该动作的得分，然后 Actor 根据 Critic 的评分修改其  选择动作的概率。其优势在于此类方法可以进行单步更新，在连续动作空间中得到得到低方差的解，而代价则是学习开始时，由于 Critic 的估计不够准确，算法有较大的波动。  </p><h2 id="多智能体强化学习"><a href="#多智能体强化学习" class="headerlink" title="多智能体强化学习"></a>多智能体强化学习</h2><h3 id="多智能体系统"><a href="#多智能体系统" class="headerlink" title="多智能体系统"></a>多智能体系统</h3><p>多智能体系统由分布式人工智能（Distributed Artificial Intelligence，DAI）演变而来，其 研究目的是解决大规模、复杂、实时和有不确定性的现实问题，此类问题通过单智能体建模往往会 效率低下并且与现实条件相悖。  </p><h2 id="博弈论"><a href="#博弈论" class="headerlink" title="博弈论"></a>博弈论</h2><p>正则式博弈， 零和博弈， 策略与策略组合， 最优反应， 纳什均衡，Minmax，Maxmin， 相关均衡， 斯塔克尔伯格均衡</p><h2 id="多智能体强化学习-1"><a href="#多智能体强化学习-1" class="headerlink" title="多智能体强化学习"></a>多智能体强化学习</h2><p><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1709009719029-68650428-9296-43f3-bf3e-be833de89f02.png#averageHue=%23f3f2f2&clientId=u9f2d59ac-852e-4&from=paste&height=486&id=u11a0f0b8&originHeight=729&originWidth=1191&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=139125&status=done&style=none&taskId=u77f3708f-d1d7-410f-892d-180827d1d74&title=&width=794"></p>]]></content>
    
    
    <categories>
      
      <category>科研</category>
      
    </categories>
    
    
    <tags>
      
      <tag>综述</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>人工智能对齐：全面性综述</title>
    <link href="/2024/02/21/%E7%BB%BC%E8%BF%B0/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AF%B9%E9%BD%90/"/>
    <url>/2024/02/21/%E7%BB%BC%E8%BF%B0/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AF%B9%E9%BD%90/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="人工智能对齐：全面性综述"><a href="#人工智能对齐：全面性综述" class="headerlink" title="人工智能对齐：全面性综述"></a>人工智能对齐：全面性综述</h1><h3 id="paper地址www-alignmentsurvey-com"><a href="#paper地址www-alignmentsurvey-com" class="headerlink" title="paper地址www.alignmentsurvey.com"></a>paper地址<a href="https://alignmentsurvey.com/">www.alignmentsurvey.com</a></h3><h3 id="talk地址https-www-bilibili-com-video-BV1Nw411t74v"><a href="#talk地址https-www-bilibili-com-video-BV1Nw411t74v" class="headerlink" title="talk地址https://www.bilibili.com/video/BV1Nw411t74v"></a>talk地址<a href="https://www.bilibili.com/video/BV1Nw411t74v">https://www.bilibili.com/video/BV1Nw411t74v</a></h3><h3 id="解读：https-www-jiqizhixin-com-articles-2023-11-01-4"><a href="#解读：https-www-jiqizhixin-com-articles-2023-11-01-4" class="headerlink" title="解读：https://www.jiqizhixin.com/articles/2023-11-01-4"></a>解读：<a href="https://www.jiqizhixin.com/articles/2023-11-01-4">https://www.jiqizhixin.com/articles/2023-11-01-4</a></h3><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><h2 id="LLMs的担忧"><a href="#LLMs的担忧" class="headerlink" title="LLMs的担忧"></a>LLMs的担忧</h2><h3 id="人工智能“不择手段”的去优化奖励函数-去Follow非中立标注者的意图来获得正向反馈"><a href="#人工智能“不择手段”的去优化奖励函数-去Follow非中立标注者的意图来获得正向反馈" class="headerlink" title="人工智能“不择手段”的去优化奖励函数,去Follow非中立标注者的意图来获得正向反馈"></a>人工智能“不择手段”的去优化奖励函数,去Follow非中立标注者的意图来获得正向反馈</h3><h2 id="RICE框架"><a href="#RICE框架" class="headerlink" title="RICE框架"></a>RICE框架</h2><p><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1708480982804-cc7a7d55-b9fb-4bcc-933c-4ef46f4384ee.png#averageHue=%23eeedec&id=k9Pcc&originHeight=1036&originWidth=1509&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="><br>鲁棒性(Robustness)、可解释性 (Interpretability)、可控性 (Controllability) 和道德性 (Ethicality)</p><h2 id="前向对齐和后向对齐"><a href="#前向对齐和后向对齐" class="headerlink" title="前向对齐和后向对齐"></a>前向对齐和后向对齐</h2><h3 id="部署前（反馈学习，分布偏移）-部署后（后向对齐验证，人工智能治理）"><a href="#部署前（反馈学习，分布偏移）-部署后（后向对齐验证，人工智能治理）" class="headerlink" title="部署前（反馈学习，分布偏移）&amp;部署后（后向对齐验证，人工智能治理）"></a>部署前（反馈学习，分布偏移）&amp;部署后（后向对齐验证，人工智能治理）</h3><h3 id="反馈学习：何对在复杂场景中运行的超级人工智能系统提供高质量的反馈，机器学习伦理"><a href="#反馈学习：何对在复杂场景中运行的超级人工智能系统提供高质量的反馈，机器学习伦理" class="headerlink" title="反馈学习：何对在复杂场景中运行的超级人工智能系统提供高质量的反馈，机器学习伦理"></a>反馈学习：何对在复杂场景中运行的超级人工智能系统提供高质量的反馈，机器学习伦理</h3><h3 id="分布偏移：目标错误泛化（不择手段获得认可），自诱发分布偏移（推荐系统，获取想要的输入反馈）。算法干预，数据分布干预"><a href="#分布偏移：目标错误泛化（不择手段获得认可），自诱发分布偏移（推荐系统，获取想要的输入反馈）。算法干预，数据分布干预" class="headerlink" title="分布偏移：目标错误泛化（不择手段获得认可），自诱发分布偏移（推荐系统，获取想要的输入反馈）。算法干预，数据分布干预"></a>分布偏移：目标错误泛化（不择手段获得认可），自诱发分布偏移（推荐系统，获取想要的输入反馈）。算法干预，数据分布干预</h3><h3 id="对齐失败，双刃剑组件"><a href="#对齐失败，双刃剑组件" class="headerlink" title="对齐失败，双刃剑组件"></a>对齐失败，双刃剑组件</h3><h1 id="从反馈中学习"><a href="#从反馈中学习" class="headerlink" title="从反馈中学习"></a>从反馈中学习</h1><p><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1708480982966-929f8890-b02d-40ef-ab12-d4d72f7468b6.png#averageHue=%23ededed&id=z3QZI&originHeight=724&originWidth=1282&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p><h2 id="反馈类型"><a href="#反馈类型" class="headerlink" title="反馈类型"></a>反馈类型</h2><h3 id="奖励：标量分数"><a href="#奖励：标量分数" class="headerlink" title="奖励：标量分数"></a>奖励：标量分数</h3><h3 id="示范：去模仿人类的某一个行为（模仿学习，IL）"><a href="#示范：去模仿人类的某一个行为（模仿学习，IL）" class="headerlink" title="示范：去模仿人类的某一个行为（模仿学习，IL）"></a>示范：去模仿人类的某一个行为（模仿学习，IL）</h3><h3 id="比较：对一系列行为排名"><a href="#比较：对一系列行为排名" class="headerlink" title="比较：对一系列行为排名"></a>比较：对一系列行为排名</h3><h3 id="如何对复杂行为定义奖励（超出人类的认知，可拓展监督），如何表达中立人类的价值观（可控性，道德性）"><a href="#如何对复杂行为定义奖励（超出人类的认知，可拓展监督），如何表达中立人类的价值观（可控性，道德性）" class="headerlink" title="如何对复杂行为定义奖励（超出人类的认知，可拓展监督），如何表达中立人类的价值观（可控性，道德性）"></a>如何对复杂行为定义奖励（超出人类的认知，可拓展监督），如何表达中立人类的价值观（可控性，道德性）</h3><h2 id="偏好建模"><a href="#偏好建模" class="headerlink" title="偏好建模"></a>偏好建模</h2><h3 id="基于比较反馈的偏好建模，偏好粒度，偏好类别（绝对，相对），奖励模型（比较反馈转化为标量）"><a href="#基于比较反馈的偏好建模，偏好粒度，偏好类别（绝对，相对），奖励模型（比较反馈转化为标量）" class="headerlink" title="基于比较反馈的偏好建模，偏好粒度，偏好类别（绝对，相对），奖励模型（比较反馈转化为标量）"></a>基于比较反馈的偏好建模，偏好粒度，偏好类别（绝对，相对），奖励模型（比较反馈转化为标量）</h3><h2 id="可扩展监督"><a href="#可扩展监督" class="headerlink" title="可扩展监督"></a>可扩展监督</h2><h3 id="可扩展监督旨在确保人工智能系统即使在超越了人类的专业知识的情况下，仍然与人类的意图保持一致"><a href="#可扩展监督旨在确保人工智能系统即使在超越了人类的专业知识的情况下，仍然与人类的意图保持一致" class="headerlink" title="可扩展监督旨在确保人工智能系统即使在超越了人类的专业知识的情况下，仍然与人类的意图保持一致"></a>可扩展监督旨在确保人工智能系统即使在超越了人类的专业知识的情况下，仍然与人类的意图保持一致</h3><h3 id="RLHF，变体RLxF-RLAIF-RLHAIF"><a href="#RLHF，变体RLxF-RLAIF-RLHAIF" class="headerlink" title="RLHF，变体RLxF,RLAIF,RLHAIF"></a>RLHF，变体RLxF,RLAIF,RLHAIF</h3><h3 id="IDA（分解任务），RRM（AI协助用户评价）Debate（两个具有分歧的AI系统对抗），CIRL（持续观察所有目标）"><a href="#IDA（分解任务），RRM（AI协助用户评价）Debate（两个具有分歧的AI系统对抗），CIRL（持续观察所有目标）" class="headerlink" title="IDA（分解任务），RRM（AI协助用户评价）Debate（两个具有分歧的AI系统对抗），CIRL（持续观察所有目标）"></a>IDA（分解任务），RRM（AI协助用户评价）Debate（两个具有分歧的AI系统对抗），CIRL（持续观察所有目标）</h3><h1 id="在分布偏移下学习"><a href="#在分布偏移下学习" class="headerlink" title="在分布偏移下学习"></a>在分布偏移下学习</h1><p><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1708480983100-023af888-59e3-45ce-a2c9-d5379761ff4e.png#averageHue=%23e6e6e6&id=rNM8I&originHeight=859&originWidth=1406&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="><br>分布偏移的挑战</p><h3 id="目标错误泛化，自诱发分布偏移"><a href="#目标错误泛化，自诱发分布偏移" class="headerlink" title="目标错误泛化，自诱发分布偏移"></a>目标错误泛化，自诱发分布偏移</h3><p>算法干预：融合多分布，分布鲁棒性优化，模式连接<br>数据分布干预：对抗训练（对抗样本），合作训练（协作学习）<br>对齐保证</p><h3 id="安全测评（鲁棒性），基准，评估目标，红队攻击"><a href="#安全测评（鲁棒性），基准，评估目标，红队攻击" class="headerlink" title="安全测评（鲁棒性），基准，评估目标，红队攻击"></a>安全测评（鲁棒性），基准，评估目标，红队攻击</h3><h3 id="可解释性，道德性"><a href="#可解释性，道德性" class="headerlink" title="可解释性，道德性"></a>可解释性，道德性</h3><h2 id><a href="#" class="headerlink" title></a></h2><h1 id="人工智能治理"><a href="#人工智能治理" class="headerlink" title="人工智能治理"></a>人工智能治理</h1><p><img src="https://cdn.nlark.com/yuque/0/2024/png/29434400/1708480983204-4d79e74f-5278-4543-9040-278f00148ebd.png#averageHue=%23f4f3f3&id=efUu6&originHeight=740&originWidth=1370&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title="></p>]]></content>
    
    
    <categories>
      
      <category>科研</category>
      
    </categories>
    
    
    <tags>
      
      <tag>综述</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>快速开始</title>
    <link href="/2024/02/21/%E6%96%87%E6%A1%A3/%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/"/>
    <url>/2024/02/21/%E6%96%87%E6%A1%A3/%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/</url>
    
    <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="08531fa066e071dcc8196d2134781ee9027d983f0e353b56ff6ea3eefa1bb9e9">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
    
    
    <categories>
      
      <category>工程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>文档</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
